\documentclass[10pt,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=true,bookmarksnumbered=true,hypertexnames=false,linkbordercolor={0 0 1}]{hyperref}
% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
%\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
%\setcounter{page}{1}
\begin{document}

%%%%%%%%% TITLE
\title{Dynamical Models for Instruction Completion \\and Error Recognition for NASA Physical Procedures}

\author{Steven Johnson\\
Department of Computer Sciences\\
University of Wisconsin--Madison\\
{\tt\small sjj@cs.wisc.edu}
\and
Ronak Mehta\\
Department of Computer Sciences\\
University of Wisconsin-Madison\\
{\tt\small ronakrm@cs.wisc.edu}
\and
John Cabaj\\
Department of Electrical and Computer Engineering\\
University of Wisconsin-Madison\\
{\tt\small cabaj@wisc.edu}
}

\maketitle

\begin{abstract}
Methods have been proposed and implemented for a variety of action and activity recognition tasks. At a larger scale, however, there has been little work on evaluating a high-level task as its individual actions are being performed. Here, we propose a method of tracking both an action being performed and its temporal location in the overall activity, using a novel combination of Hidden Markov Models and Petri Nets. We implement our method in the context of instruction completion, where someone may be performing a task being observed with an egocentric camera.

\end{abstract}

\section{Introduction}

Procedures are the accepted means to operate a spacecraft system or systems to perform specific functions, and consequently are at the heart of all NASA human spaceflight operations~\cite{kortenkamp2008procedure}. A procedure is a detailed set of instructions specifying how a piece of equipment is operated or a task is performed~\cite{frank2010plans}. They are often written to be very general and to cover numerous contingencies. Procedures to operate a class of equipment (e.g., smoke detector) will differ based on make, while procedures to operate a piece of equipment will have conditional or optional steps based on configuration. As an additional complication, constraints of some procedures may be highly conditional, discretionary, or unordered. At the same time, there may be external constraints that limit how a procedure must be executed, and these constraints are not made explicit. The outcomes of NASA missions rely on crew members properly executing a multitude of these complex procedures, making procedure execution support and monitoring a critical factor that can determine success or failure measured both in terms of monetary costs as well as preventing loss of life.

There is a body of prior NASA work focused on monitoring the progress of procedures that are not physical. For instance, when instructions to systems of the ISS are sent from ground, the application ThinLayer highlights commands as they are executed to show procedure progress~\cite{frank2010plans}. IPV itself also allows for manually tracking procedure progress for a crew person onboard ISS. However, to date there is little work from NASA in the realm of tracking execution status of physical procedures where crew members are manually manipulating physical objects, such as during maintenance tasks. Our goal with this project is to develop a method to computationally model a procedure to enable tracking of the execution of its steps and detection of crew errors during execution.

The inputs to our system will be a set of videos of users correctly executing one procedure (an exercise equipment maintenance task) recorded from a head-mounted egocentric camera. Using this set of videos, we will develop a technique for learning a dynamical model of the procedure that extends current methods by incorporating domain knowledge from the provided procedure documentation. We will then evaluate the model on a set of videos which are both correct and contain errors to determine the accuracy of error detection and overall instruction segmentation.

\section{Related Work}

Significant work has been done in the field of human action and activity recognition. In \cite{turaga2008machine}, Turaga presents a comprehensive overview of this work in detail. In most work, activity recognition is identified as the sum of \emph{actions} performed in a temporal ordering. Parametric models, particularly Hidden Markov Models, have been used with success in many action recognition applications. \cite{yamato1992recognizing} employ them to identify whole-body tennis swings. Using background subtraction, they are able to identify the actor in the scene, and learn a model based on how the actor alone is moving over time. The particular advantage of HMMs in the field of vision lies in their efficiency in modeling time-sequential data. Here, we also have no knowledge of the state except through our extracted features, which fits well into the unobserved state representation of HMMs.
	
In order to properly specify the given task, it is necessary to maintain several levels of abstraction to gain an overall representation of the procedure. It is intuitive to use three levels: task, activity, and actions where the task is the overall goal of the procedure, activity is the sub-activities making up the task, and actions are the lowest level for each activity. To this end, Petri Nets\cite{petri1966communication} are an obvious choice, given the parallel drawn between actions on an object, and transition actions which change the state of the object. 

Petri Nets offer a more intuitive approach to states and actions, and can be represented using a bi-partite graph\cite{turaga2008machine}. The states of the Petri Net correspond well to our model of tasks, sub-activites, and sub-actions. The transitions in this case are actions that have been returned from action recognition using an HMM. When a state is entered, the transitions emanating from the given state are enabled, and further advancement in the Petri Net can be controlled with more abstract granularity. In this way, the Petri Net will allow a strict limitation of the steps that can be executed given a current configuration of the object being operated on. This draws a parallel with the sequential nature of instruction completion.

In order to provide action transitions, a proper representation of the action must be attained. Feature extraction must be performed to determine object representation at a low-level. \cite{fathi2011understanding} describe a method in which to identify objects as well as some coarse actions which aid in object identification. Given the object representation, we can use a Hidden Markov Model (HMM) to determine the likely action based on the objects in frame and the current action state. This provides a spatio-temporal model in which to classify the low-level actions making up the activity level Petri Nets.

The entire representation of a task as a whole is broken up into two overall levels: a Petri Net level to provide a framework for probable next actions, and a Hidden Markov Model with underlying feature representation to define low-level activities. We combine both levels to address the shortcomings of each. Petri Nets tend to only allow for more abstract actions and are difficult to train, while HMMs are more limited to recognition of a specific action and can't maintain an overall sense of the state of the task as a whole.
  
\section{Problem Description and Techniques You Want to Use}

While our project falls in the general space of activity recognition, our specific task includes key domain knowledge that we plan to leverage. Most of the work mentioned above describes tasks in which a single body is in motion against a known background with a fixed camera. Here, an ego-centric camera is used, and tasks involve the manipulation of objects within the scene. Not only is it necessary to be able to distinguish the action currently taken place, but objects in the space must be tracked as they are being manipulated. These objects may be occluded, combined, or separated throughout the task. A key part of our project will be creating a strong feature representation to extract from the raw video. \cite{starner1998real} provide us with a strong starting point, as they use prior information about relevant objects in the scene that allow them to quickly extract these objects from irrelevant background. 

Once a feature representation has been extracted, a model must be created that will allow us to learn the task in question. There has been some previous work on temporal modeling of time-variant tasks as mentioned above, but with mediocre success. Again, we will use \cite{starner1998real} as our starting point, given the similar underlying task. Here is where we attempt to incorporate our domain knowledge, in the hope that we may be able to create a stronger model.

Our higher level activity recognition may require a significantly different model than that used for the lower-level action recognition. Because each "activity" may be correct or incorrect, and may occur in different temporal orderings, a model that incorporates logic will be necessary. Petri-Nets have been used by \cite{castel1996going}, because of their natural representation of sequencing, parallelism, and synchronization. All of these will prove to be valuable in our domain, and may allow us to better model the nature of step-by-step assembly.

\section{Experimental Evaluation}
 
Because of the difficulty in gathering training data, we will focus our efforts for modeling and evaluation to a single task instance of an exercise equipment verification task. This task was selected because it is similar to many tasks executed by actual crew members on ISS. The experimental data will be gathered by recruiting participants to wear a Google Glass device loaded with an application to guide them through the task. One of the authors has access to the resources necessary to gather the data. Thus, we will obtain many egocentric videos of a user executing the entire task. We will hand-segment each of these recordings according to the actions we define from the procedure document and are encoded in our petri net, and an HMM will be constructed from each of these actions.

Our evaluation will be twofold: we wish to both verify that the models we generate are able to identify when actions are completed in a video we did not use in the training set, and additionally we wish to see if the model can detect errors in a new video which includes errors. To do the first part, for each video, we will train the model on all of the training data except for that video, and we will then examine the action segmentation results of evaluating the model on that video. Hopefully we will see accurate detection of the completion of actions. For the second part, we will require additional videos, some of which contain errors in the task. We will evaluate the model on this set of videos, hopefully seeing accurate (and timely) detection of errors in the videos which contain errors and not see false positives in the videos which are error-free.
 
\section{Conclusion and Discussion}

Currently, work will be distributed evenly across all three components of the project: feature extraction, action modeling, and activity modeling. We will initially start with concurrent work on the feature extraction and activity modeling components. Once feature extraction is complete, action modeling with HMMs will be implemented. Data will be collected during this period, and integration between action and activity modeling will be the final step.

\clearpage

{\small
\bibliographystyle{ieee}
\bibliography{proposal}
}

\end{document}
